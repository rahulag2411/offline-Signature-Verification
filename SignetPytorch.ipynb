{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SignetPytorch.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "BEuTsRQPZ3EO"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import csv\n",
        "import os\n",
        "import glob\n",
        "import random\n",
        "import itertools\n",
        "from typing import List, Tuple\n",
        "\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import ImageOps\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchvision.transforms as transforms\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import shutil\n",
        "\n",
        "import time"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fhiZN_P_TE-9"
      },
      "source": [
        "def make_partition(signers: List[int],pairGenuineGenuine: List[Tuple[int, int]],pairGenuineForged: List[Tuple[int, int]]):\n",
        "    samples = []\n",
        "    for signer_id in signers:\n",
        "        genuineGenuine = list(itertools.zip_longest(pairGenuineGenuine, [], fillvalue=1)) # y = 1\n",
        "        genuineGenuine  = list(map(lambda sample: (signer_id, *sample[0], sample[1]), genuineGenuine ))\n",
        "        samples.extend(genuineGenuine )\n",
        "\n",
        "        subPairGenuineForged = random.sample(pairGenuineForged, len(pairGenuineGenuine))\n",
        "        genuineForged = list(itertools.zip_longest(subPairGenuineForged, [], fillvalue=0)) # y = 0\n",
        "        genuineForged= list(map(lambda sample: (signer_id, *sample[0], sample[1]), genuineForged))\n",
        "        samples.extend(genuineForged)\n",
        "    return samples"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Iok4I7l2Tbum"
      },
      "source": [
        "def prepare_CEDAR(M: int, K: int, random_state=0, data_dir='/content/drive/MyDrive/SigNet Implementation/offlineSignatureVerification/data/CEDAR'):\n",
        "    def get_path(row):\n",
        "        signer_id, x1, x2, y = row\n",
        "        if y == 1:\n",
        "            x1 = os.path.join(data_dir, 'full_org', f'original_{signer_id}_{x1}.png')\n",
        "            x2 = os.path.join(data_dir, 'full_org', f'original_{signer_id}_{x2}.png')\n",
        "        else:\n",
        "            x1 = os.path.join(data_dir, 'full_org', f'original_{signer_id}_{x1}.png')\n",
        "            x2 = os.path.join(data_dir, 'full_forg', f'forgeries_{signer_id}_{x2}.png')\n",
        "        return x1, x2, y # drop signer_id\n",
        "\n",
        "    random.seed(random_state)\n",
        "    signers = list(range(1, K+1))\n",
        "    num_genuine_sign = 24\n",
        "    num_forged_sign = 24\n",
        "\n",
        "    train_signers, test_signers = train_test_split(signers, test_size=K-M)\n",
        "    pairGenuineGenuine = list(itertools.combinations(range(1, num_genuine_sign+1), 2))\n",
        "    pairGenuineForged = list(itertools.product(range(1, num_genuine_sign+1), range(1, num_forged_sign+1)))\n",
        "    \n",
        "\n",
        "    train_samples = make_partition(train_signers, pairGenuineGenuine, pairGenuineForged)\n",
        "    \n",
        "    train_samples = list(map(get_path, train_samples))\n",
        "    \n",
        "    train_file_path = os.path.join(data_dir, 'train.csv')\n",
        "    with open(train_file_path, 'wt') as f:\n",
        "        writer = csv.writer(f)\n",
        "        writer.writerows(train_samples)\n",
        "   \n",
        "    test_samples = make_partition(test_signers, pairGenuineGenuine, pairGenuineForged)\n",
        "    test_samples = list(map(get_path, test_samples))\n",
        "    \n",
        "    test_file_path = os.path.join(data_dir, 'test.csv')\n",
        "    with open(test_file_path, 'wt') as f:\n",
        "        writer = csv.writer(f)\n",
        "        writer.writerows(test_samples)\n",
        "        \n",
        "seed = 2021\n",
        "np.random.seed(seed)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pblgq4NHZ86Q"
      },
      "source": [
        "class SignDataset(Dataset):\n",
        "    def __init__(self, is_train: bool, data_dir: str, image_transform=None):\n",
        "        self.image_transform = image_transform\n",
        "        if is_train:\n",
        "            self.df = pd.read_csv(os.path.join(data_dir, 'train.csv'), header=None)\n",
        "        else:\n",
        "            self.df = pd.read_csv(os.path.join(data_dir, 'test.csv'), header=None)\n",
        "        \n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        x1, x2, y = self.df.iloc[index]\n",
        "\n",
        "        x1 = Image.open(x1).convert('L')\n",
        "        x2 = Image.open(x2).convert('L')\n",
        "        \n",
        "        if self.image_transform:\n",
        "            x1 = self.image_transform(x1)\n",
        "            x2 = self.image_transform(x2)\n",
        "\n",
        "        return x1, x2, y\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IMU1gjfraEc3"
      },
      "source": [
        "class SigNetModel(nn.Module):\n",
        "    def __init__(self):\n",
        "            super().__init__()\n",
        "            \n",
        "            self.features = nn.Sequential(\n",
        "            #input size = [155, 220, 1]\n",
        "            nn.Conv2d(in_channels = 1, out_channels = 96, kernel_size = 11), # size = [145,210,96]\n",
        "            nn.ReLU(),\n",
        "            nn.LocalResponseNorm(size=5, k=2, alpha=1e-4, beta=0.75),\n",
        "            nn.MaxPool2d(2, stride=2), # size = [72, 105,96]\n",
        "                \n",
        "            nn.Conv2d(in_channels = 96, out_channels = 256, kernel_size = 5, padding=2, padding_mode='zeros'), # size = [72, 105,256]\n",
        "            nn.LocalResponseNorm(size=5, k=2, alpha=1e-4, beta=0.75),\n",
        "            nn.MaxPool2d(2, stride=2), # size = [36, 52,256]\n",
        "            nn.Dropout2d(p=0.3),\n",
        "                \n",
        "            nn.Conv2d(in_channels = 256, out_channels = 384,kernel_size = 3, stride=1, padding=1, padding_mode='zeros'),\n",
        "            nn.Conv2d(in_channels = 384, out_channels = 256,kernel_size = 3, stride=1, padding=1, padding_mode='zeros'),\n",
        "            nn.MaxPool2d(2, stride=2), # size = [18, 26,256]\n",
        "            nn.Dropout2d(p=0.3),\n",
        "                \n",
        "            nn.Flatten(1, -1), # 18*26*256\n",
        "            nn.Linear(18*26*256, 1024),\n",
        "            nn.Dropout2d(p=0.5),\n",
        "            nn.Linear(1024, 128),\n",
        "            )\n",
        "            \n",
        "    def forward(self, x1, x2):\n",
        "        x1 = self.features(x1)\n",
        "        x2 = self.features(x2)\n",
        "        return x1, x2\n",
        "        "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BR0FMfjEaJfw"
      },
      "source": [
        "image_transform = transforms.Compose([\n",
        "        transforms.Resize((155, 220)),\n",
        "        ImageOps.invert,\n",
        "        transforms.ToTensor(),\n",
        "        \n",
        "    ])\n",
        "\n",
        "prepare_CEDAR(M = 50,K = 55)\n",
        "\n",
        "train_data = SignDataset(is_train = True, data_dir = \"/content/drive/MyDrive/SigNet Implementation/offlineSignatureVerification/data/CEDAR\",image_transform =  image_transform)\n",
        "test_data = SignDataset(is_train = False, data_dir = \"/content/drive/MyDrive/SigNet Implementation/offlineSignatureVerification/data/CEDAR\",image_transform =  image_transform)\n",
        "\n",
        "loaders = {\n",
        "    'train_loader' : DataLoader(train_data, batch_size=32, shuffle=True, num_workers=2, pin_memory=True),\n",
        "    'test_loader'  : DataLoader(test_data, batch_size=32, shuffle=False, num_workers=2, pin_memory=True),\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t5yhZZtraMEf"
      },
      "source": [
        "class ContrastiveLoss(nn.Module):\n",
        "    def __init__(self, alpha, beta, margin):\n",
        "        super().__init__()\n",
        "        self.alpha = alpha\n",
        "        self.beta = beta\n",
        "        self.margin = margin\n",
        "\n",
        "    def forward(self, x1, x2, y):\n",
        "        distance = torch.pairwise_distance(x1, x2, p=2)\n",
        "        loss = self.alpha * (1-y) * distance**2 + \\\n",
        "               self.beta * y * (torch.max(torch.zeros_like(distance), self.margin - distance)**2)\n",
        "        return torch.mean(loss, dtype=torch.float)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1bpXIkMsaQs-"
      },
      "source": [
        "def accuracy(distances, y, step=0.01):\n",
        "    min_threshold_d = min(distances)\n",
        "    max_threshold_d = max(distances)\n",
        "    max_acc = 0\n",
        "    same_id = (y == 1)\n",
        "\n",
        "    for threshold_d in torch.arange(min_threshold_d, max_threshold_d+step, step):\n",
        "        true_positive = (distances <= threshold_d) & (same_id)\n",
        "        true_positive_rate = true_positive.sum().float() / same_id.sum().float()\n",
        "        true_negative = (distances > threshold_d) & (~same_id)\n",
        "        true_negative_rate = true_negative.sum().float() / (~same_id).sum().float()\n",
        "\n",
        "        acc = 0.5 * (true_negative_rate + true_positive_rate)\n",
        "        max_acc = max(max_acc, acc)\n",
        "    return max_acc\n",
        "\n",
        "@torch.no_grad()\n",
        "def eval(model, criterion, dataloader, log_interval=40):\n",
        "    model.eval()\n",
        "    running_loss = 0\n",
        "    number_samples = 0\n",
        "\n",
        "    distances = []\n",
        "\n",
        "    for batch_idx, (x1, x2, y) in enumerate(dataloader):\n",
        "        x1, x2, y = x1.to('cuda'), x2.to('cuda'), y.to('cuda')\n",
        "\n",
        "        x1, x2 = model(x1, x2)\n",
        "        loss = criterion(x1, x2, y)\n",
        "        distances.extend(zip(torch.pairwise_distance(x1, x2, 2).cpu().tolist(), y.cpu().tolist()))\n",
        "\n",
        "        number_samples += len(x1)\n",
        "        running_loss += loss.item() * len(x1)\n",
        "\n",
        "        if (batch_idx + 1) % 40 == 0 or batch_idx == len(dataloader) - 1:\n",
        "            print('{}/{}: Loss: {:.4f}'.format(batch_idx+1, len(dataloader), running_loss / number_samples))\n",
        "\n",
        "    distances, y = zip(*distances)\n",
        "    distances, y = torch.tensor(distances), torch.tensor(y)\n",
        "    max_accuracy = accuracy(distances, y)\n",
        "    print(f'Max accuracy: {max_accuracy}')\n",
        "    return running_loss / number_samples, max_accuracy\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mdV36l5zaSui",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "38bfd2e7-72bf-479b-fe7e-1c6e0ab57f1a"
      },
      "source": [
        "model = SigNetModel().to(\"cuda\")\n",
        "criterion = ContrastiveLoss(alpha=1, beta=1, margin=1).to(\"cuda\")\n",
        "optimizer = optim.RMSprop(model.parameters(), lr=1e-5, eps=1e-8, weight_decay=5e-4, momentum=0.9)\n",
        "scheduler = optim.lr_scheduler.StepLR(optimizer, 5, 0.1)\n",
        "num_epochs = 20\n",
        "model.train()\n",
        "print(model)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "SigNetModel(\n",
            "  (features): Sequential(\n",
            "    (0): Conv2d(1, 96, kernel_size=(11, 11), stride=(1, 1))\n",
            "    (1): ReLU()\n",
            "    (2): LocalResponseNorm(5, alpha=0.0001, beta=0.75, k=2)\n",
            "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (4): Conv2d(96, 256, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
            "    (5): LocalResponseNorm(5, alpha=0.0001, beta=0.75, k=2)\n",
            "    (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (7): Dropout2d(p=0.3, inplace=False)\n",
            "    (8): Conv2d(256, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (9): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (10): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (11): Dropout2d(p=0.3, inplace=False)\n",
            "    (12): Flatten(start_dim=1, end_dim=-1)\n",
            "    (13): Linear(in_features=119808, out_features=1024, bias=True)\n",
            "    (14): Dropout2d(p=0.5, inplace=False)\n",
            "    (15): Linear(in_features=1024, out_features=128, bias=True)\n",
            "  )\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BBX8SHFWrNSs"
      },
      "source": [
        "def saveCheckpoint(state, isBest, checkpointPath, bestModelPath):\n",
        "    fPath = checkpointPath\n",
        "    torch.save(state, fPath)\n",
        "    if isBest:\n",
        "        bestFPath = bestModelPath\n",
        "        shutil.copyfile(fPath, bestFPath)\n",
        "\n",
        "def loadCheckpoint(checkpointFPath, model, optimizer, scheduler):\n",
        "    checkpoint = torch.load(checkpointFPath)\n",
        "    model.load_state_dict(checkpoint['model'])\n",
        "    optimizer.load_state_dict(checkpoint['optim'])\n",
        "    scheduler.load_state_dict(checkpoint['scheduler'])\n",
        "    return model, optimizer, checkpoint['epoch'], scheduler"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PmRSGfukvlYy"
      },
      "source": [
        "def trainInternal(model, optimizer, criterion, dataloader, log_interval=50):\n",
        "    model.train()\n",
        "    running_loss = 0\n",
        "    number_samples = 0\n",
        "\n",
        "    for batch_idx, (x1, x2, y) in enumerate(dataloader):\n",
        "        x1, x2, y = x1.to('cuda'), x2.to('cuda'), y.to('cuda')\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        x1, x2 = model(x1, x2)\n",
        "        loss = criterion(x1, x2, y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        number_samples += len(x1)\n",
        "        running_loss += loss.item() * len(x1)\n",
        "        if (batch_idx + 1) % log_interval == 0 or batch_idx == len(dataloader) - 1:\n",
        "            print('{}/{}: Loss: {:.4f}'.format(batch_idx+1, len(dataloader), running_loss / number_samples))\n",
        "            running_loss = 0\n",
        "            number_samples = 0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1m70LyWIZtw-"
      },
      "source": [
        "def trainModel(start_epochs, num_epochs, loaders, model, optimizer, criterion, checkpointPath, bestModelPath):\n",
        "  \n",
        "  for epoch in range(start_epochs, num_epochs):\n",
        "    print('Epoch {}/{}'.format(epoch, num_epochs))\n",
        "    print('Training', '-'*20)\n",
        "        \n",
        "    trainInternal(model, optimizer, criterion, loaders['train_loader'], log_interval=50)\n",
        "    print('Evaluating', '-'*20)\n",
        "    loss, acc = eval(model, criterion, loaders['test_loader'])\n",
        "    scheduler.step()\n",
        "\n",
        "    to_save = {\n",
        "        'epoch': epoch + 1,\n",
        "        'model': model.state_dict(),\n",
        "        'scheduler': scheduler.state_dict(),\n",
        "        'optim': optimizer.state_dict(),    \n",
        "        }\n",
        "    print('Saving checkpoint..')\n",
        "    saveCheckpoint(to_save, False, checkpointPath, bestModelPath)\n",
        "    torch.save(to_save, '/content/drive/MyDrive/SigNet Implementation/offlineSignatureVerification/PYTORCHCHECKPOINT/epoch_{}_loss_{:.3f}_acc_{:.3f}.pt'.format(epoch, loss, acc))\n",
        "\n",
        "  return model\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PP8YkmlpixTC",
        "outputId": "c025b229-830d-455b-c6fc-36130d6b84d1"
      },
      "source": [
        "model.train()\n",
        "start_time = time.time()\n",
        "trained_model = trainModel(0, 20, loaders, model, optimizer, criterion, \"/content/drive/MyDrive/SigNet Implementation/offlineSignatureVerification/CHECKPOINT/currentCheckPoint.pt\", \"/content/drive/MyDrive/SigNet Implementation/offlineSignatureVerification/best_model/bestModel.pt\")\n",
        "end_time = time.time()\n",
        "print(\"Model trained....\")\n",
        "print(\"Training Time: {}\".format((end_time-start_time)/60))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dA6urjPLnqEa"
      },
      "source": [
        "#resume model training\n",
        "\n",
        "model = SigNetModel().to(\"cuda\")\n",
        "optimizer = optim.RMSprop(model.parameters(), lr=1e-5, eps=1e-8, weight_decay=5e-4, momentum=0.9)\n",
        "scheduler = optim.lr_scheduler.StepLR(optimizer, 5, 0.1)\n",
        "checkpointFPath = '/content/drive/MyDrive/SigNet Implementation/offlineSignatureVerification/checkpoint/currentCheckPoint.pt'\n",
        "model, optimizer, start_epoch, scheduler = loadCheckpoint(checkpointFPath, model, optimizer, scheduler)\n",
        "\n",
        "print(\"model = \", model)\n",
        "print(\"optimizer = \", optimizer)\n",
        "print(\"start_epoch = \", start_epoch)\n",
        "print(\"scheduler = \", scheduler)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IqbZCwj6pbxq"
      },
      "source": [
        "start_time3 = time.time()\n",
        "trained_model = train(start_epoch, 20, loaders, model, optimizer, criterion, \"/content/drive/MyDrive/SigNet Implementation/offlineSignatureVerification/checkpoint/currentCheckPoint.pt\", \"/content/drive/MyDrive/SigNet Implementation/offlineSignatureVerification/best_model/bestModel.pt\")\n",
        "end_time3 = time.time()\n",
        "print(\"Model trained....\")\n",
        "print(\"Training Time: {}\".format((end_time3-start_time3)/60))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SKxtHojuaWMO"
      },
      "source": [
        "# start_time = time.time()\n",
        "# for epoch in range(num_epochs):\n",
        "#     print('Epoch {}/{}'.format(epoch, num_epochs))\n",
        "#     print('Training', '-'*20)\n",
        "    \n",
        "#     model.train()\n",
        "    \n",
        "#     running_loss = 0\n",
        "#     number_samples = 0\n",
        "    \n",
        "#     for batch_idx, (x1, x2, y) in enumerate(train_loader):\n",
        "#         x1, x2, y = x1.to('cuda'), x2.to('cuda'), y.to('cuda')\n",
        "\n",
        "#         optimizer.zero_grad()\n",
        "#         x1, x2 = model(x1, x2)\n",
        "#         loss = criterion(x1, x2, y)\n",
        "#         loss.backward()\n",
        "#         optimizer.step()\n",
        "\n",
        "#         number_samples += len(x1)\n",
        "#         running_loss += loss.item() * len(x1)\n",
        "#         if (batch_idx + 1) % 40 == 0 or batch_idx == len(train_loader) - 1:\n",
        "#             print('{}/{}: Loss: {:.4f}'.format(batch_idx+1, len(train_loader), running_loss / number_samples))\n",
        "#             running_loss = 0\n",
        "#             number_samples = 0\n",
        "#     scheduler.step()\n",
        "#     loss, acc = eval(model, criterion, test_loader)\n",
        "#     to_save = {\n",
        "#             'model': model.state_dict(),\n",
        "#             'scheduler': scheduler.state_dict(),\n",
        "#             'optim': optimizer.state_dict(),\n",
        "#         }\n",
        "#     print('Saving checkpoint..')\n",
        "#     torch.save(to_save, 'checkpoints/epoch_{}_loss_{:.3f}_acc_{:.3f}.pt'.format(epoch, loss, acc))\n",
        "\n",
        "\n",
        "# end_time = time.time()\n",
        "# print(\"Training Time: {}\".format((end_time-start_time)/60))\n",
        "\n",
        "# start_time1 = time.time()\n",
        "# print('Evaluating', '-'*20)\n",
        "# loss, acc = eval(model, criterion, test_loader)\n",
        "# end_time1 = time.time()\n",
        "\n",
        "# print(\"Evaluation Time: {}\".format((end_time1-start_time1)/60))\n",
        "\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZvHjrmB11IJ9"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
